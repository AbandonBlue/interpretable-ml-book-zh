# 第六章 基于实例的解释

基于示例的解释方法选择数据集的特定实例来解释机器学习模型的行为或解释基础数据分布。

基于实例的解释大多是模型不可知论，因为它们使任何机器学习模型更易于解释。与模型不可知方法的区别在于，基于示例的方法通过选择数据集的实例来解释模型，而不是通过创建特性摘要（如或）来解释模型。基于实例的解释只有在我们能够以人性化的方式表示数据的实例时才有意义。这对图像很有效，因为我们可以直接查看它们。一般来说，如果实例的特征值包含更多的上下文，这意味着数据具有像图像或文本那样的结构，那么基于示例的方法可以很好地工作。以有意义的方式表示表格数据更具挑战性，因为一个实例可以包含数百或数千个（不太结构化）特性。列出所有用于描述实例的特性值通常不有用。如果只有一小部分特性，或者我们有一种方法来总结一个实例，那么它会很好地工作。

基于实例的解释有助于人类构建机器学习模型的心理模型和机器学习模型所训练的数据。它尤其有助于理解复杂的数据分布。但我所说的基于实例的解释是什么意思呢？我们经常在工作和日常生活中使用它们。让我们从一些例子开始。

医生看到病人有不寻常的咳嗽和轻微的发烧。病人的症状使她想起了几年前的另一个病人，症状相似。她怀疑她现在的病人可能患有同一种疾病，并采集血样来检测这种特定的疾病。

一位数据科学家正在为他的一位客户进行一项新的项目：分析导致键盘生产机器故障的风险因素。数据科学家记得他所做的一个类似的项目，并重用旧项目中的部分代码，因为他认为客户需要相同的分析。

一只小猫坐在一座燃烧的无人居住的房子的窗台上。消防队已经到了，其中一名消防队员想了想，他是否可以冒险进入大楼去救小猫。作为一名消防员，他还记得类似的情况：燃烧缓慢一段时间的旧木屋经常不稳定，最终倒塌。因为这个案子的相似性，他决定不进去，因为房子倒塌的风险太大了。幸运的是，这只小猫跳出窗户，安全着陆，没有人在火灾中受伤。幸福的结局。

这些故事用例子或类比来说明我们人类是如何思考的。基于实例的解释的蓝图是：事物B与事物A和a引起的y相似，所以我预测b也会引起y。隐含地，一些机器学习方法是基于工作实例的。根据预测目标的重要特征中数据点的相似性将数据划分为节点。决策树通过查找相似的实例（=在同一终端节点中）并返回这些实例结果的平均值作为预测值来获取新数据实例的预测。k-最近邻（knn）方法与基于实例的预测明确工作。对于一个新的实例，knn模型定位k-最近的邻居（例如k=3个最近的实例），并返回这些邻居结果的平均值作为预测。knn的预测可以通过返回k邻居来解释，同样，只有当我们有一个很好的方法来表示一个实例时，k邻居才有意义。

本部分各章包括以下基于实例的解释方法：

•  告诉我们一个实例必须如何改变才能显著改变其预测。通过创建反事实的实例，我们了解了模型是如何做出预测的，并且可以解释个别的预测。

•  是用来愚弄机器学习模型的反事实。重点是推翻预测，而不是解释它。

•  是从数据中选择的代表性实例，而批评是那些原型不能很好地表示的实例。

•  是对预测模型参数或预测本身影响最大的训练数据点。识别和分析有影响的实例有助于发现数据问题，调试模型并更好地了解模型的行为。

•  ：基于实例的（可解释的）机器学习模型。

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image001.gif)

\1.    Aamodt、Agnar和Enric广场。“基于案例的推理：基础问题、方法变化和系统方法。”人工智能通信7.1（1994）：39-59。

\2.    Kim、Be、Rajiv Khanna和Oluwasanmi O.Koyejo。“例子不够，要学会批判！对可解释性的批评〉，《神经信息处理系统的进展》（2016年）。

6.1反事实解释

一个反事实的解释描述了一种因果关系的形式：“如果x没有发生，y就不会发生”。例如：“如果我不喝这杯热咖啡，我就不会把舌头烧伤。”事件Y是我烧了舌头；因为X是我喝了一杯热咖啡。反事实思维需要想象一个与观察到的事实相矛盾的假想现实（例如，一个我没有喝热咖啡的世界），因此得名“反事实”。在反事实方面思考的能力使我们人类比其他动物更聪明。

在可解释的机器学习中，反事实的解释可以用来解释个别情况的预测。“事件”是一个实例的预测结果，“原因”是输入到模型中的该实例的特定特征值，“原因”是一个特定的预测。以图形的形式显示，输入和预测之间的关系非常简单：特征值导致预测。

图6.1：机器学习模型输入与预测之间的因果关系，当模型仅被视为一个黑匣子时。输入导致预测（不一定反映数据的真正因果关系）。

即使在现实中，输入和要预测的结果之间的关系可能不是因果关系，我们也可以将模型的输入视为预测的原因。

在这个简单的图中，很容易看出我们如何模拟机器学习模型预测的反事实：我们只需在进行预测之前更改实例的特征值，然后分析预测是如何变化的。我们对预测以相关方式发生变化的情景感兴趣，例如预测类的翻转（例如，接受或拒绝信贷申请）或预测达到某一阈值（例如，癌症的概率达到10%）。对预测的反事实解释描述了将预测更改为预定义输出的特征值的最小更改。

反事实解释方法是模型不可知论，因为它只适用于模型的输入和输出。这种方法也会让人感到宾至如归。

解释可以表示为特征值差异的总结（“更改特征A和B以更改预测”）。但是反事实的解释本身就是一个新的例子，所以它活在这一章中（“从实例x开始，改变a和b以获得反事实的实例”）。与反事实不同，不必是训练数据中的实际实例，而是可以是特征值的新组合。

在讨论如何创建反事实之前，我想讨论一些反事实的用例，以及一个好的反事实解释是怎样的。

在第一个例子中，Peter申请贷款，并被（机器学习驱动的）银行软件拒绝。他想知道为什么他的申请被拒绝，以及如何提高他获得贷款的机会。“为什么”的问题可以被表述为一个反事实：将预测从拒绝变为批准的特征（收入、信用卡数量、年龄等）的最小变化是什么？一个可能的答案是：如果彼得每年多赚1万欧元，他就会得到贷款。或者，如果彼得的信用卡更少，而且5年前没有拖欠贷款，他就会得到贷款。彼得永远不会知道拒绝的原因，因为银行对透明度没有兴趣，但那是另一回事。

在我们的第二个例子中，我们要解释一个模型，它用反事实的解释来预测一个连续的结果。安娜想租出她的公寓，但她不知道要多少钱，所以她决定训练一个机器学习模型来预测租金。当然，因为安娜是一个数据科学家，所以她就是这样解决她的问题的。在输入了所有关于尺寸、位置、是否允许养宠物等细节后，模特告诉她可以收取900欧元的费用。她期望1000欧元或更多，但她相信她的模型，并决定发挥公寓的功能价值，看看她如何提高公寓的价值。她发现这套公寓如果再大1500万欧元，可以以1000多欧元的价格出租。有趣，但非2个可操作的知识，因为她不能扩大她的公寓。最后，通过只调整她控制下的功能值（内置厨房是/否、允许宠物是/否、地板类型等），她发现如果她允许宠物并安装隔热性能更好的窗户，她可以收取1000欧元。安娜凭直觉用反事实来改变结果。

反事实是，因为它们与当前实例形成对比，而且它们是选择性的，这意味着它们通常只关注少量的特性更改。但是，反事实也会受到“拉森效应”的影响。《罗生门》是一部日本电影，其中一个武士的谋杀案是由不同的人讲述的。每一个故事都同样很好地解释了结果，但这些故事相互矛盾。反事实也可能发生同样的情况，因为通常有多种不同的反事实解释。每一个反事实都讲述了一个不同的“故事”，讲述了某个结果是如何达到的。一个反事实可以说改变了特征A，另一个反事实可以说留下了相同的，但改变了特征B，即A

矛盾。这个多个事实的问题可以通过报告所有的反事实的解释来解决，也可以通过有一个标准来评估反事实并选择最好的一个。

说到标准，我们如何定义一个好的反事实解释？首先，反事实解释的用户在一个实例的预测中定义了一个相关的变化（=替代现实），因此一个明显的首要要求是反事实实例尽可能地生成预先定义的预测。不可能总是完全匹配预定义的输出。在一个包含两个类、一个稀有类和一个频繁类的分类环境中，模型总是可以将一个实例分类为频繁类。更改特性值，使预测标签从普通类翻转到罕见类可能是不可能的。因此，我们希望放宽这样一个要求，即反事实的预测结果必须与定义的结果完全对应。在分类示例中，我们可以寻找一个反事实，其中稀有类的预测概率增加到10%，而不是当前的2%。那么问题是，特征的最小变化是什么，使得预测概率从2%变为10%（或接近10%）。另一个质量标准是，关于特征值，反事实应该尽可能类似于实例。这需要在两个实例之间测量距离。反事实不仅要接近原案，而且要尽可能少地改变特征。这可以通过选择适当的距离测量来实现，如曼哈顿距离。最后一个要求是反事实实例应该具有可能的特征值。如果一套公寓的大小为负数或房间数设置为200，那么就没有必要对租金示例做出反事实的解释。根据数据的联合分布，如10间房20米的公寓不应被视为反事实的解释，当反事实可能出现时，情况会更好。

6.1.1产生反事实解释

一种简单而幼稚的产生反事实解释的方法是通过试错法进行搜索。这种方法包括随机更改感兴趣的实例的特征值，并在预测所需的输出时停止。比如安娜试图找到一套公寓的版本，她可以收取更多的租金。但是有比尝试和错误更好的方法。首先，我们定义了一个损失函数，它将利益实例、反事实和期望（反事实）结果作为输入。损失衡量反事实的预测结果与预先确定的结果之间的距离，以及反事实与利益实例之间的距离。我们可以使用优化算法直接优化损失，也可以通过搜索实例来优化损失，如“增长球体”方法（请参见）。

在本节中，我将介绍Wachter等人（2017）建议的方法，他们建议将以下损失降至最低。

\[L（x，x^ \ prime，y^ \ prime，lambda）=\lambda\cdot（\hat f（x^ \ prime）-y^ \ prime）^2+d（x，x^ \ prime）\]

第一项是模型预测反事实x'和期望结果y'之间的二次距离，用户必须提前定义。第二个术语是要解释的实例x和反事实x'之间的距离d’，但稍后会详细介绍。参数\（\lambda\）平衡预测中的距离（第一项）和特征值中的距离（第二项）。对给定的\（\lambda\）求解损失，并返回反事实x'。较高的值\（\lambda\）意味着我们更喜欢接近期望结果y的反事实，较低的值意味着我们更喜欢特征值中与x非常相似的反事实x。如果\（\lambda\）非常大，无论预测距离x多远，都将选择预测最接近y'的实例。最终，用户必须决定如何平衡反事实预测与所需结果匹配的要求。t反事实类似于x。该方法的作者建议不要为\（\lambda\）选择一个值，而是选择一个公差\（\epsilon\）来决定反事实实例的预测距离y'的距离。这个约束可以写为：

\[\hat f（x ^ \prime）-y ^ \prime \leq \epsilon \]

为了最小化这个损失函数，可以使用任何合适的优化算法，例如Nelder Mead。如果您可以访问机器学习模型的渐变，那么您可以使用基于渐变的方法，如Adam。要解释的实例x、所需的输出y'和公差参数\（\epsilon\）必须提前设置。对于x'和（局部）最佳反事实x'的损失函数最小化，同时增加\（\lambda\）直到找到足够接近的解（=在公差参数内）。

\[\arg\min_x^ \ prime \ max \ lambda l（x，x^ \ prime，y^ \ prime，lambda）\]

用于测量实例x和反事实x'之间距离的函数d是曼哈顿距离加权特征，具有反向中值绝对偏差（MAD）。

\[d（x，x^ \ prime）=\sum j=1 ^p \ frac x j-x ^ \ prime mad j \]

总距离是所有P向特征距离的总和，即实例x和反事实x'之间特征值的绝对差异。特征距离通过特征j在数据集上的中值绝对偏差的倒数缩放，定义为：

\[mad_j=\text中位数1，ldots，n（x i，j-\text中位数1，ldots，n（x l，j）]]

向量的中位数是向量值的一半大，另一半小的值。MAD是特征方差的等价物，但我们不使用平均值作为中心，而使用平方距离求和，而是使用中间值作为中心，并在绝对距离求和。与欧氏距离相比，距离函数具有引入稀疏性的优点。这意味着当较少的特征不同时，两个点之间的距离更近。对于离群值来说，它更强大。使用MAD进行缩放是必要的，以使所有功能达到相同的比例——无论您是以平方米还是平方英尺为单位测量公寓的大小都不重要。

制作反事实的方法很简单：

\1.    选择要解释的实例x、所需的结果y、公差（epsilon）和（low）初始值（lambda）。

\2.    抽样一个随机的实例作为初始反事实。

\3.    以初始抽样反事实为出发点，对损失进行优化。

\4.    而\（\hat f（x ^ \ prime）-y ^ \ prime>\epsilon\）：

增加\（\lambda\）。

以当前反事实为起点优化损失。退回减少损失的反事实。

\5.    重复步骤2-4并返回反事实列表或最小化损失的列表。

6.1.2示例

这两个例子都来自Wachter等人（2017年）的研究。

在第一个例子中，作者训练了一个三层完全连接的神经网络，根据法学院入学前的平均成绩（GPA）、种族和法学院入学考试成绩，预测法学院学生的第一年平均成绩。目标是为每个回答以下问题的学生找到反事实的解释：输入特性需要如何改变，才能得到0的预测分数？因为分数以前已经被标准化了，所以分数为0的学生和学生的平均分一样好。负分表示低于平均分的结果，正分表示高于平均分的结果。

下表显示了所学的反事实：

## ScoregpalSatracegpa x'Lsat x'race x'

| 0.17 3.1条  | 39.0分 | 第3.1条 | 34.0条   | 0    |
| ----------- | ------ | ------- | -------- | ---- |
| 0.54 3.7条  | 48.0分 | 第3.7条 | 第32.4条 | 0    |
| -0.77 3.3条 | 28.0 1 | 第3.3条 | 33.5条   | 0    |
| -0.83 2.4条 | 28.5一 | 第2.4条 | 35.8条   | 0    |
| -0.57 2.7条 | 18.3 0 | 2.7条   | 第34.9条 | 0    |

第一列包含预测得分，接下来的3列包含原始特征值，最后3列包含导致得分接近0的反事实特征值。前两行是预测高于平均水平的学生，其他三行低于平均水平。前两行的反事实描述了如何改变学生的特征以降低预测分数，另外三个案例则描述了如何改变才能将分数提高到平均水平。提高分数的反事实总是将种族从黑色（用1编码）变为白色（用0编码），这显示了模型的种族偏见。在反事实中，GPA没有改变，但是LSAT改变了。

第二个例子显示了预测糖尿病风险的反事实解释。一个三层的完全连接的神经网络被训练来预测糖尿病的风险，这取决于年龄，体重指数，怀孕次数等等。反事实回答了这个问题：哪些特征值必须改变才能将糖尿病的风险评分提高或降低到0.5？发现以下反事实：

•  人1：如果你的2小时血清胰岛素水平为154.3，你将得到0.51分。

•  人2：如果你的2小时血清胰岛素水平为169.5，你将得到0.51分。

•  人3：如果你的血糖浓度是158.3，你的2小时血清胰岛素水平是160.5，你将得到0.51分。

6.1.3优势

反事实解释的解释非常清楚。如果根据反事实更改实例的特征值，则预测将更改为预定义的预测。没有额外的假设，背景中也没有魔力。这也意味着它不像其他方法那样危险，因为我们还不清楚可以在多大程度上推断出解释的局部模型。

反事实方法创建了一个新的实例，但是我们也可以通过报告哪些特征值发生了变化来总结反事实。这为我们提供了两个报告结果的选项。您可以报告反事实实例，也可以突出显示感兴趣的实例和反事实实例之间已更改的功能。

反事实方法不需要访问数据或模型。它只需要访问模型的预测函数，例如，它也可以通过Web API工作。对于由第三方审计的公司或在不披露模型或数据的情况下为用户提供解释的公司来说，这是很有吸引力的。由于商业秘密或数据保护原因，公司有兴趣保护模型和数据。反事实解释在解释模型预测和保护模型所有者利益之间提供了平衡。

该方法也适用于不使用机器学习的系统。我们可以为任何接收输入和返回输出的系统创建反事实。预测公寓租金的系统也可能包括手写规则，而反事实的解释仍然有效。

反事实解释方法相对容易实现，因为它本质上是一个损失函数，可以用标准优化器库进行优化。必须考虑一些额外的细节，例如将特征值限制在有意义的范围内（例如，只有正的公寓尺寸）。

6.1.4缺点

对于每一个例子，你通常会发现多个反事实的解释（拉森蒙效应）。这是不方便的——大多数人喜欢简单的解释，而不是现实世界的复杂性。这也是一个实际的挑战。例如，我们生成了23个反事实的解释。我们都报告了吗？只有最好的？如果它们都是相对“好”的，但是非常不同呢？对于每个项目，必须重新回答这些问题。有多个反事实的解释也是有利的，因为这样人类就可以选择与他们以前的知识相对应的解释。

对于给定的公差（epsilon），不能保证找到了反事实的实例。这不一定是方法的错误，而是取决于数据。

该方法不能很好地处理多层次的分类特征。该方法的作者建议对分类特征的每个特征值组合分别运行该方法，但是如果您有多个具有多个值的分类特征，这将导致组合爆炸。例如，6个具有10个唯一级别的分类功能意味着100万次运行。Martens等人（2014年）提出了仅针对分类特征的解决方案。一个好的解决方案是使用优化器来解决连续和离散输入混合的问题。

反事实方法缺乏通用的软件实现。方法只有在实现时才有用。幸运的是，它应该很容易实现，希望我可以很快删除这条语句。

6.1.5软件和备选方案

不幸的是，目前没有可用于反事实解释的软件。

Martens等人（2014）提出了一种非常类似的方法来解释文件分类。在他们的工作中，他们重点解释为什么一个文档被或没有被归类为一个特定的类。与本章介绍的方法不同的是，Martens等人（2014年）将重点放在文本分类器上，文本分类器将单词出现作为输入。

另一种搜索反事实的方法是Laugel等人（2017）的生长球体算法。该方法首先在兴趣点周围绘制一个球体，在该球体内采样点，检查其中一个采样点是否产生所需的预测、收缩或扩展SPH。因此，在找到（稀疏的）反事实并最终返回之前。他们在论文中没有使用“反事实”这个词，但方法非常相似。它们还定义了一个有利于反事实的损失函数，并且尽可能少地改变特征值。

他们不直接优化函数，而是建议使用球体进行上述搜索。

图6.2:Laugel等人关于生长球体和选择稀疏反事实的说明。

Al（2017年）。

Ribeiro等人（2018）的锚与反事实相反。锚回答问题：哪些特征足以锚定预测，即更改其他特征不能更改预测？一旦我们找到了作为预测锚定的特性，我们将不再通过改变锚定中未使用的特性来发现反事实的实例。

图6.3:Ribeiro等人（2018年）的锚示例。

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image002.gif)

\1.    瓦赫特、桑德拉、布伦特·米特斯塔特和克里斯·拉塞尔。“不打开黑匣子的反事实解释：自动决策和GDPR。”（2017年）。

\2.    马丁斯、大卫和福斯特教务长。“解释数据驱动文档分类。”（2014年）。

[↩](https://christophm.github.io/interpretable-ml-book/counterfactual.html#fnref46)

\3.    Laugel，Thibault等人“机械学习中基于比较的可解释性的逆向分类”，ARXIV预印ARXIV:1712.08443（2017）。

\4.    Ribeiro、Marco Tulio、Sameer Singh和Carlos Guestrin。“锚：高精度模型不可知论解释”，AAAI人工智能会议（2018）。

6.2对抗性示例

一个敌对的例子是一个小的，故意的特征干扰，导致机器学习模型作出错误的预测的例子。我建议先阅读这一章，因为它们的概念非常相似。对抗性的例子是反事实的例子，目的是欺骗模型，而不是解释它。

为什么我们对敌对的例子感兴趣？他们不只是好奇的机器学习模型的副产品没有实际意义吗？答案是明确的“不”。对抗性示例使机器学习模型容易受到攻击，如以下场景中所示。

一辆自动驾驶的汽车撞到另一辆车上，因为它忽略了停车标志。有人在标志上放了一张图片，它看起来像一个停车标志，对人类来说有点脏，但设计得像一个停车禁止标志，用于汽车的标志识别软件。

垃圾邮件检测器无法将电子邮件分类为垃圾邮件。垃圾邮件被设计成类似于普通邮件，但目的是欺骗收件人。

一台机器学习型扫描仪在机场扫描手提箱中的武器。为了避免被发现，开发了一把刀，使系统认为它是一把伞。

让我们来看一些创建对抗性示例的方法。

6.2.1方法和示例

有许多技巧可以创建对抗性的例子。大多数方法都建议在将预测转移到期望的（对抗的）结果的同时，将对抗性实例和要操纵的实例之间的距离最小化。有些方法需要访问模型的梯度，当然这些方法只适用于基于梯度的模型，如神经网络，其他方法只需要访问预测函数，这使得这些方法模型不可知。本节中的方法主要集中在基于深度神经网络的图像分类器上，在这方面做了大量的研究，对敌方图像的可视化非常有教育意义。图像的对抗性示例是带有有意扰动像素的图像，目的是在应用期间欺骗模型。这些例子令人印象深刻地展示了深层神经网络如何容易被对人类无害的图像欺骗。如果你还没有看到这些例子，你可能会感到惊讶，因为对人类观察者来说，预测的变化是不可理解的。相反的例子就像是光学错觉，但对机器来说是如此。

## 我的狗出毛病了

Szegedy等人（2013年）在他们的工作“神经网络的有趣特性”中使用了基于梯度的优化方法来寻找深层神经网络的对抗性示例。

图6.4:Szegedy等人（2013年）对Alexnet的敌对示例。左列中的所有图像都已正确分类。中间一列显示添加到图像中的（放大的）错误，以生成右一列中的图像，所有分类（错误）为“鸵鸟”。

这些相反的例子是通过最小化与r有关的以下函数而产生的：

\[损失（\hat f（x+r），l）+c\cdot r \]

在这个公式中，x是一个图像（表示为像素向量），r是对像素的更改以创建一个对抗图像（x+r生成一个新图像），l是所需的结果类，参数c用于平衡图像之间的距离和预测之间的距离。第一项是对抗性实例的预测结果与期望的L类之间的距离，第二项是测量对抗性实例与原始图像之间的距离。该公式几乎与要生成的损失函数相同。R还有其他约束，因此像素值保持在0和1之间。作者建议用盒约束的L-BFGS来解决这个优化问题，这是一种适用于梯度的优化算法。

## 扰动熊猫：快速梯度标记法

Goodfellow等人（2014年发明了用于生成对抗图像的快速梯度符号方法。

梯度符号法利用底层模型的梯度来寻找对立的例子。

原始图像X是通过在每个像素上加上或减去一个小的错误来操作的。我们加或减\（\epsilon\）取决于像素的渐变符号是正的还是负的。在梯度方向上增加误差意味着图像被有意修改，从而导致模型分类失败。

图6.5:Goodfellow等人（2014年）在神经网络中使熊猫看起来像长臂猿。通过将小扰动（中间图像）添加到原始熊猫像素（左图像）中，作者创建了一个敌对的例子，它被归类为长臂猿（右图像），但在人类看来像熊猫。

下面的公式描述了快速梯度符号方法的核心内容：

其中\（\bigstriangledown_x j\）是模型损失函数相对于原始输入像素向量x的梯度，y是x的真标签向量，而\（\theta\）是模型参数向量。从梯度向量（与输入像素的向量一样长）我们只需要符号：如果像素强度增加损失（模型产生的误差），梯度符号为正（+1），如果像素强度减少损失，则为负（-1）。当神经网络线性处理输入像素强度和类分数之间的关系时，就会出现此漏洞。特别是，倾向于线性化的神经网络结构，如LSTMS、MAXOUT网络、带有RELU激活单元的网络或其他线性机器学习算法（如逻辑回归）易受梯度符号法的影响。攻击是通过外推法进行的。输入像素强度和类分数之间的线性关系导致易受异常值的影响，即通过将像素值移动到数据分布之外的区域，可以欺骗模型。我期望这些对抗性的例子对于给定的神经网络体系结构是非常具体的。但事实证明，你可以重复使用敌对的例子来欺骗在同一个任务上训练了不同体系结构的网络。

Goodfellow等人（2014）建议在培训数据中添加对抗性示例，以学习健壮的模型。

## 水母……不，等等。浴缸：1像素攻击

Goodfellow及其同事（2014年）提出的方法需要改变很多像素，即使只是稍微改变一点。但是如果你只能改变一个像素呢？你能欺骗一个机器学习模型吗？Su等人（2019年）表明，通过改变单个像素实际上可以欺骗图像分类器。

图6.6：通过故意改变一个像素（用圆圈标记），在ImageNet上训练的神经网络被欺骗，从而预测错误的类而不是原始类。Su等人（2019）的研究。

类似于反事实，1像素攻击寻找一个修改过的示例x'接近原始图像x，但将预测改为对抗结果。然而，亲密度的定义不同：只有一个像素可能会改变。1像素攻击使用差分进化来找出要更改的像素以及如何更改。物种的生物进化松散地激发了差异进化。一个被称为候选解决方案的群体一代一代地重组，直到找到解决方案。每个候选解决方案编码一个像素修改，并由五个元素的向量表示：x和y坐标以及红、绿和蓝（rgb）值。例如，搜索从400个候选解决方案（=像素修改建议）开始，并使用以下公式从父代创建新一代候选解决方案（子代）：

\[X I（G+1）=X R1（G）+F\CDOT（X R2（G）+X R3（G））]

其中每个（x_i）是候选解的元素（x坐标、y坐标、红色、绿色或蓝色），g是当前生成的，f是缩放参数（设置为0.5），r1、r2和r3是不同的随机数。每个新的子候选解决方案依次是一个像素，其中包含五个位置和颜色属性，每个属性都是三个随机父像素的混合。

如果一个候选解决方案是一个对立的示例，意味着它被分类为不正确的类，或者达到了用户指定的最大迭代次数，则停止创建子级。

## 一切都是一台烤面包机：敌对的补丁

我最喜欢的方法之一就是把敌对的例子带到现实中。Brown等人（2017）设计了一个可打印标签，可以贴在物体旁边，使其看起来像图像分类器的烤面包机。干得漂亮！

图6.7：一个贴纸，它使在ImageNet上训练的VGG16分类器将香蕉的图像分类为烤面包机。Brown等人的工作（2017年）。

这种方法不同于迄今为止针对对抗性实例所提出的方法，因为消除了对抗性图像必须非常接近原始图像的限制。相反，该方法将图像的一部分完全替换为可以呈现任何形状的补丁。补丁的图像在不同的背景图像上进行了优化，补丁在图像上的位置不同，有时会移动，有时会变大或变小并旋转，因此补丁可以在许多情况下工作。最后，该优化后的图像可以在野外打印并用于欺骗图像分类器。

## 千万不要把3D打印的乌龟带到枪战中——即使你的电脑认为这是个好主意：强大的对抗性例子

下一种方法是在烤面包机上添加另一个维度：Athalye等人（2017）3印刷了一只乌龟，从几乎所有可能的角度来看，它看起来就像一把步枪。是的，你读对了。对人类来说，一个看起来像乌龟的物体在计算机上看起来就像一把步枪！

图6.8：一只3D打印乌龟，被TensorFlow的标准预先训练识别为步枪。

inceptionv3分类器。Athalye等人的工作（2017年）

作者发现了一种在3D中为二维分类器创建对抗性示例的方法，这种分类器在转换过程中具有对抗性，例如旋转乌龟、放大等所有可能性。其他方法，如快速梯度法，在图像旋转或视角变化时不再有效。Athalye等人（2017）提出了期望过度转换（EOT）算法，这是一种生成对抗性示例的方法，甚至在图像转换时也能工作。EOT背后的主要思想是通过许多可能的转换来优化敌对的例子。在给定可能变换的选定分布的情况下，EOT不会将敌对示例和原始图像之间的距离最小化，而是将两个示例之间的预期距离保持在某个阈值以下。转换下的预期距离可写为：

\[\mathbb e t\sim t[d（t（x^\prime），t（x））]\

其中x是原始图像，t（x）是转换后的图像（例如旋转），x'是敌对的例子，t（x'）是转换后的版本。除了处理转换的分布之外，eot方法还遵循熟悉的模式，将搜索敌对示例作为优化问题进行框架化。我们试图找到一个对抗性的例子x'来最大化所选类（y_t）的概率（例如“来复枪”）在可能的转换分布t：

\[\arg\max_x ^ \prime \mathbb e t \sim t[日志p（y_t t（x ^ \prime））]\]

在限制条件下，敌对示例x'和原始图像x之间所有可能转换的预期距离保持在某个阈值以下：

\[\mathbb e t\sim t[d（t（x^\prime），t（x））]<\epsilon\quad\text and \quad x \in[0,1]^d\]

我认为我们应该关注这种方法的可能性。其他方法是基于数字图像的处理。然而，这些3D打印的、强大的对抗性示例可以插入任何真实场景中，并欺骗计算机对对象进行错误分类。让我们把它转过来：如果有人制造了一把看起来像乌龟的步枪怎么办？

## 蒙蔽的对手：黑匣子攻击

想象一下下面的场景：我让您通过Web API访问我的优秀图像分类器。您可以从模型中获得预测，但是您没有访问模型参数的权限。从您的沙发方便，您可以发送数据和我的服务答案与相应的分类。大多数的对抗性攻击并不能在这种情况下工作，因为它们需要访问底层深层神经网络的梯度来找到对抗性的例子。Papernot和同事（2017年）表明，在没有内部模型信息和无法访问培训数据的情况下，可以创建具有对抗性的示例。这种（几乎）零知识攻击称为黑箱攻击。

工作原理：

\1.    从一些与训练数据来自同一域的图像开始，例如，如果要攻击的分类器是数字分类器，则使用数字图像。需要了解该领域的知识，但不需要访问培训数据。

\2.    从黑盒中获取当前图像集的预测。

\3.    在当前图像集（例如神经网络）上训练代理模型。

\4.    使用启发式方法创建一组新的合成图像，该方法检查当前图像集，在哪个方向上操作像素以使模型输出具有更大的方差。

\5.    对预定义的epoch数重复步骤2到4。

\6.    使用快速梯度法（或相似法）为代理模型创建对抗性示例。

\7.    用敌对的例子攻击原始模型。

代理模型的目的是近似黑箱模型的决策边界，但不一定达到相同的精度。

作者通过攻击在各种云机器学习服务上训练的图像分类器来测试这种方法。这些服务在用户上传的图像和标签上训练图像分类器。软件会自动训练模型（有时使用用户未知的算法），并部署它。然后，分类器对上传的图像进行预测，但无法检查或下载模型本身。作者能够为不同的提供者找到对抗性的例子，多达84%的对抗性的例子被错误分类。

如果要欺骗的黑盒模型不是神经网络，这种方法甚至可以工作。这包括没有梯度的机器学习模型，例如决策树。

6.2.2网络安全观

机器学习处理已知的未知：从已知的分布预测未知的数据点。对攻击的防御处理未知的未知：从未知的敌方输入分布中可靠地预测未知的数据点。随着机器学习被集成到越来越多的系统中，例如自动车辆或医疗设备，它们也成为攻击的入口点。即使机器学习模型在测试数据集上的预测是100%正确的，也可以发现相反的例子来欺骗模型。针对网络攻击的机器学习模型防御是网络安全领域的一个新的组成部分。

Biggio等人（2018年）对本节所依据的对抗性机器学习的十年研究进行了很好的回顾。网络安全是一场军备竞赛，在这场竞赛中，攻击者和防御者一次又一次地相互竞争。

网络安全有三条黄金法则：1）了解你的对手2）积极主动3）保护你自己。

不同的应用程序有不同的对手。试图通过电子邮件骗取他人钱财的人是用户和电子邮件服务提供商的对手代理。提供者想要保护他们的用户，这样他们就可以继续使用他们的邮件程序，攻击者想让人们给他们钱。了解你的对手意味着了解他们的目标。假设您不知道这些垃圾邮件发送者存在，并且电子邮件服务的唯一滥用是发送盗版音乐副本，那么防御将有所不同（例如，扫描附件以获取受版权保护的材料，而不是分析文本中的垃圾邮件指标）。

积极主动意味着积极测试和识别系统的弱点。当你积极地试图用敌对的例子欺骗模型，然后进行防御时，你是主动的。使用解释方法来了解哪些特征是重要的，以及特征如何影响预测，也是了解机器学习模型弱点的一个积极步骤。作为数据科学家，你是否相信你的模型在这个危险的世界里，而从来没有超越过对测试数据集的预测能力？您是否分析了模型在不同场景中的行为，确定了最重要的输入，检查了一些示例的预测解释？你有没有试着找到对抗性的输入？机器学习模型的可解释性在网络安全中起着重要作用。与主动性相反，被动性意味着等待系统被攻击，然后才了解问题并安装一些防御措施。

我们如何保护我们的机器学习系统不受敌对例子的影响？一种主动的方法是对分类器进行反复的再培训，其中包含对抗性示例，也称为对抗性训练。其他方法是基于博弈论的，例如学习特征的不变变换或鲁棒优化（正则化）。另一种方法是使用多个分类器，而不是一个分类器，让他们投票预测（合集），但这并不能保证工作，因为他们都可能遭受类似的敌对例子。另一个也不太好用的方法是梯度屏蔽，它通过使用最近邻分类器而不是原始模型来构造一个没有有用梯度的模型。

我们可以通过攻击者对系统的了解程度来区分攻击类型。攻击者可能具有完善的知识（白盒攻击），这意味着他们了解模型的类型、参数和训练数据等所有信息；攻击者可能具有部分知识（灰盒攻击），这意味着他们可能只知道模型的特征表示和类型。已使用的模型，但无法访问培训数据或参数；攻击者可能没有任何知识（黑盒攻击），这意味着他们只能以黑盒方式查询模型，但无法访问培训数据或模型参数信息。根据信息的级别，攻击者可以使用不同的技术攻击模型。正如我们在示例中看到的，即使在黑盒情况下，也可以创建敌对的示例，因此隐藏有关数据和模型的信息不足以抵御攻击。

考虑到攻击者和防御者之间的猫捉老鼠游戏的性质，我们将在这一领域看到许多发展和创新。想想不断演变的许多不同类型的垃圾邮件。提出了针对机器学习模型的新攻击方法，并针对这些新攻击提出了新的防御措施。更强大的攻击是为了躲避最新的防御等，无限期。通过这一章，我希望能够使您对对抗性例子的问题更加敏感，并且只有通过积极地研究机器学习模型，我们才能发现并弥补弱点。

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image003.gif)

\1.    Szegedy，Christian等人“神经网络的有趣特性”，arxiv预印arxiv:1312.6199（2013）。

\2.    Goodfellow、Ian J、Jonathon Shlens和Christian Szegedy。“解释和利用个人示例”，arxiv预印arxiv:1412.6572（2014）。

\3.    苏、加威、达尼洛·瓦斯卡塞洛斯·瓦加斯和樱井。“愚弄深度神经网络的单像素攻击”，《IEEE进化计算汇刊》（2019年）。

\4.    Brown，Tom B.等人“对抗补丁。”arxiv预印arxiv:1712.09665（2017）。

\5.    阿塔利、安尼和伊利亚·萨茨基弗。“合成强大的对抗性示例”，arxiv预印arxiv:1707.07397（2017）。

\6.    Papernot，Nicolas等人“针对机器学习的实际黑盒攻击”，2017年ACM亚洲计算机与通信安全会议进程。ACM（2017年）。

\7.    Biggio、Battista和Fabio Roli。“野性模式：对抗性机器学习兴起十年后”，《模式识别》84（2018）：317-331。

6.3原型和批评

原型是代表所有数据的数据实例。批评是一个数据实例，它不能很好地由一组原型表示。批评的目的是与原型一起提供见解，特别是对于原型不能很好地表示的数据点。原型和批评可以独立于机器学习模型来描述数据，但它们也可以用于创建可解释模型或使黑盒模型可解释。

在本章中，我使用表达式“数据点”来指代单个实例，以强调对实例也是坐标系中的一个点的解释，其中每个特征都是一个维度。下图显示了一个模拟的数据分布，其中一些实例被选为原型，一些实例被选为批评。小点是数据，大点是原型，大点是批评。选择原型（手动）以覆盖数据分布的中心，批评是没有原型的集群中的点。原型和批评总是来自数据的实际实例。

图6.9：具有两个特性x1和x2的数据分布的原型和批评。

我手动选择了原型，它的缩放效果不好，可能导致效果不佳。有许多方法可以在数据中找到原型。其中之一是k-medoid，一种与k-means算法相关的聚类算法。任何将实际数据点作为聚类中心返回的聚类算法都有资格选择原型。但这些方法中的大多数只能找到原型，但没有批评。本章介绍了Kim等人（2016）提出的MMD批评家，一种将原型和批评结合在一个框架中的方法。

MMD批评家比较了数据的分布和所选原型的分布。这是理解MMD批评家方法的核心概念。MMD批评家选择的原型可以最小化两个分布之间的差异。高密度区域的数据点是很好的原型，尤其是从不同的“数据集群”中选择点时。从原型无法很好解释的区域中选取数据点作为批评。

让我们深入研究这个理论。

6.3.1理论

可以简要总结高层次的MMD批评程序：

\1.    选择要查找的原型和批评的数量。

\2.    用贪婪的搜索寻找原型。选择原型是为了使原型的分布接近数据分布。

\3.    贪婪地寻找批评。当原型的分布不同于数据的分布时，选择点作为批评。

我们需要一些成分来找到一个数据集的原型和批评与MMD批评家。作为最基本的成分，我们需要一个核函数来估计数据密度。内核是一个函数，它根据两个数据点的接近程度来衡量它们的权重。基于密度估计，我们需要一个度量来告诉我们两个分布有多不同，这样我们就可以确定我们选择的原型的分布是否接近数据分布。这可以通过测量最大平均偏差（mmd）来解决。另外，基于内核函数，我们需要见证函数来告诉我们在一个特定的数据点上两个分布有多不同。利用见证函数，我们可以选择批评，即原型和数据分布偏离的数据点，见证函数具有较大的绝对值。最后一个要素是对好的原型和批评的搜索策略，它通过一个简单的贪婪搜索来解决。

让我们从最大平均差（mmd）开始，它测量两个分布之间的差异。选择原型可以创建原型的密度分布。我们要评估原型分布是否与数据分布不同。我们用核密度函数来估计两者。最大平均差度量两个分布之间的差异，即根据两个分布，期望值之间差异的函数空间上的上确界。全部清除？就我个人而言，当我看到数据是如何计算的时，我更理解这些概念。下面的公式显示了如何计算平方mmd测量值（mmd2）：

\[mmd^2=\frac 1 m^2 \ sum i，j=1 k（z i，z j）-\frac 2 m n \ sum i，j=1 m，n k（z i，x j）+\frac 1

_n^2 \总和i，j=1 ^n k（x_i，x_j）]

k是一个核心函数，用于测量两点的相似性，但稍后将详细介绍。m是原型z的数量，n是原始数据集中的数据点x的数量。原型Z是数据点X的选择。每个点都是多维的，也就是说它可以有多个特性。MMD批评家的目标是最小化MMD2。mmd2越接近于零，原型的分布越符合数据。将mmd2降为零的关键是中间的术语，它计算原型和所有其他数据点之间的平均接近度（乘以2）。如果这个术语加上第一个术语（原型彼此之间的平均接近度）加上最后一个术语（数据彼此之间的平均接近度），那么原型就完美地解释了数据。尝试一下如果使用所有n个数据点作为原型，公式会发生什么情况。

下图说明了mmd2测量值。第一个图显示了具有两个特征的数据点，其中数据密度的估计以阴影背景显示。每个其他绘图显示不同的原型选择，以及绘图标题中的mmd2度量。原型是大点，它们的分布显示为轮廓线。选择最能覆盖这些场景中数据的原型（左下角）具有最小的差异值。

图6.10：具有两个特征和不同原型选择的数据集的平方最大平均差异度量（mmd2）。

内核的选择是径向基函数kernel:\[k（x，x^\prime）=exp\left（\gamma x-x^\prime ^2\right）\]

其中x-x'是两点之间的欧几里得距离，而\（\gamma\）是一个缩放参数。2.当两点之间的距离无穷远时，内核值随两点之间的距离而减小，范围在0和1之间：当两点之间无限远时为零；当两点相等时为一。

我们将mmd2度量、内核和贪婪搜索结合在一个查找原型的算法中：

•  从一个空的原型列表开始。

•  当原型数量低于所选数量m时：

对于数据集中的每个点，检查将该点添加到原型列表时MMD2减少了多少。将最小化mmd2的数据点添加到列表中。

•  返回原型列表。

找到批评的剩余要素是见证函数，它告诉我们在一个特定点上两个密度估计值有多少不同。可通过以下方式估算：

\[见证人（x）=\frac 1 n \ sum i=1 ^nk（x，x i）-\frac 1 m \ sum j=1 ^mk（x，z j）\]

对于两个数据集（具有相同的特性），见证函数为您提供了一种评估方法，在这种方法中，经验分布X点更适合。为了找到批评，我们从消极和积极两个方面寻找证人功能的极端价值。见证函数中的第一项是X点和原型之间的平均邻近度，第二项分别是X点和数据之间的平均邻近度。如果X点的见证函数接近于零，则数据和原型的密度函数紧密相连，这意味着原型的分布类似于X点的数据分布。X点的正见证函数意味着原型的分布Tion高估了数据分布（例如，如果我们选择了一个原型，但附近只有很少的数据点）；X点的负见证函数意味着原型分布低估了数据分布（例如，如果X附近有很多数据点，但我们有e未选择附近的任何原型）。

为了给你更多的直觉，让我们先用最低的mmd2重用图中的原型，并显示几个手动选择的点的见证函数。下图中的标签显示了标记为正方形的各个点的见证函数值。只有中间的一点具有很高的绝对价值，因此是一个很好的批评候选人。

图6.11：不同点见证功能的评估。

见证函数允许我们显式地搜索原型不能很好地表示的数据实例。批评是证人功能中绝对值较高的点。和原型一样，批评也是通过贪婪的搜索发现的。但是，我们并没有减少整个MMD2，而是在寻找能够最大化成本函数的点，其中包括见证函数和正则化项。优化函数中的附加项加强了点的多样性，这是为了使点来自不同的簇。

第二步与如何找到原型无关。我也可以选择一些原型并使用这里描述的过程来学习批评。或者原型可以来自任何聚类过程，比如k-medoid。

这就是《货币市场部评论理论》的重要组成部分。还有一个问题：MMD批评家如何被用于可解释的机器学习？

MMD批评家可以通过三种方式添加可解释性：帮助更好地理解数据分布；通过构建可解释模型；通过使黑盒模型可解释。

如果您将MMD批评家应用于您的数据以查找原型和批评，它将提高您对数据的理解，特别是如果您具有复杂的边缘案例数据分布。但有了MMD批评家，你可以实现更多！

例如，您可以创建一个可解释的预测模型：所谓的“最近的原型模型”。预测函数定义为：

\[\hat f（x）=argmax i \ in s k（x，x i）\]

这意味着我们从最接近新数据点的一组原型中选择原型I，从这个意义上说，它产生了内核函数的最高值。原型本身作为预测的解释返回。此过程有三个调整参数：内核类型、内核缩放参数和原型数量。所有参数都可以在交叉验证循环中进行优化。这种方法不使用批评。

作为第三种选择，我们可以使用MMD批评家，通过检查原型和批评以及它们的模型预测，使任何机器学习模型都可以全球解释。程序如下：

\1.    与MMD评论家一起寻找原型和评论。

\2.    像往常一样训练机器学习模型。

\3.    用机器学习模型预测原型和批评的结果。

\4.    分析预测：在哪种情况下算法是错误的？现在，您有一些很好地表示数据的示例，帮助您发现机器学习模型的弱点。

这有什么帮助？还记得谷歌的图像分类器识别出黑人是大猩猩吗？也许他们应该在部署图像识别模型之前使用这里描述的过程。仅仅检查模型的性能是不够的，因为如果它是99%正确的，这个问题可能仍然在1%之内。标签也可能是错误的！检查所有的训练数据，并做一个健全的检查，如果预测有问题，可能已经揭示了问题，但将是不可行的。但是，选择——比如说几千个——原型和批评是可行的，并且可能揭示了数据的一个问题：它可能表明缺乏皮肤黝黑的人的图像，这表明数据集的多样性存在问题。或者它可以展示一个或多个以深色皮肤为原型的人的图像，或者（可能）是对臭名昭著的“大猩猩”分类的批评。我不保证MMD批评家一定会截获这些错误，但这是一个良好的理智检查。

6.3.2示例

我从MMD评论家的论文中举了一些例子。这两个应用程序都基于图像数据集。每幅图像都用2048维的图像嵌入来表示。图像嵌入是一个带有数字的向量，它捕获图像的抽象属性。嵌入向量通常是从神经网络中提取出来的，神经网络经过训练可以解决图像识别任务，在这种情况下，图像网络的挑战。使用这些嵌入向量计算图像之间的核距离。

第一个数据集包含来自ImageNet数据集的不同狗品种。MMD批评家应用于两个犬类的数据。在狗的左边，原型通常显示狗的脸，而批评是没有狗的脸或不同颜色（如黑白）的图像。在右侧，原型包含了狗的户外图像。批评包括穿着服装的狗和其他不寻常的案例。

图6.12：来自ImageNet数据集的两种狗品种的原型和批评。

MMD评论家的另一个例子使用手写数字数据集。

查看实际的原型和批评，您可能会注意到每个数字的图像数量是不同的。这是因为在整个数据集中搜索了固定数量的原型和批评，而不是每个类使用固定数量。正如预期的那样，原型显示了不同的数字写入方式。批评包括线条粗细异常的例子，也包括无法识别的数字。

图6.13：手写数字数据集的原型和批评。

6.3.3优势

在一项用户研究中，MMD批评家的作者给参与者提供了图片，他们必须在视觉上与两组图片中的一组相匹配，每一组图片代表两个类别中的一个（例如两个狗品种）。当一组学生展示原型和批评时，他们表现得最好，而不是一个班级的随机图片。

你可以自由选择原型和批评的数量。

MMD批评家处理数据的密度估计。这适用于任何类型的数据和任何类型的机器学习模型。

该算法易于实现。

MMD批评家在提高可解释性方面非常灵活。它可以用来理解复杂的数据分布。它可以用来建立一个可解释的机器学习模型。也可以为黑匣子机器学习模型的决策提供依据。

发现批评与原型的选择过程无关。但是根据MMD评论家选择原型是有意义的，因为原型和批评都是使用相同的方法来比较原型和数据密度创建的。

6.3.4缺点

你必须选择原型和批评的数量。尽管这样做很好，但也是一个缺点。我们实际需要多少原型和批评？越多越好？越少越好？一种解决方案是通过测量人类观察图像的时间来选择原型和批评的数量，这取决于特定的应用程序。只有在使用MMD批评家构建分类器时，我们才能直接对其进行优化。一种解决方案可以是显示X轴上原型数量和Y轴上mmd2测量值的屏幕图。我们将选择mmd2曲线变平的原型数量。

其他参数是内核和内核缩放参数的选择。我们和原型和批评的数量有同样的问题：我们如何选择内核及其缩放参数？同样，当我们使用mmd critical作为最近的原型分类器时，我们可以调整内核参数。然而，对于MMD批评家的无监督使用案例，还不清楚。（也许我在这里有点苛刻，因为所有无监督的方法都有这个问题。）

它将所有特性作为输入，忽略了一些特性可能与预测感兴趣的结果无关的事实。一种解决方案是只使用相关功能，例如图像嵌入而不是原始像素。只要我们有一种方法将原始实例投影到只包含相关信息的表示上，这就可以工作。

有一些代码是可用的，但是它还没有被很好地打包和文档化的软件实现。

6.3.5规范和备选方案

可以在以下位置找到MMD批评家的实现：。

找到原型最简单的替代方法是Kaufman等人（1987年）。

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image004.gif)

\1.    Kim、Be、Rajiv Khanna和Oluwasanmi O.Koyejo。“例子不够，要学会批判！对可解释性的批评〉，《神经信息处理系统的进展》（2016年）。

\2.    考夫曼、伦纳德和彼得·卢梭。“通过类药物聚集”。北荷兰（1987）。

6.4影响因素

机器学习模型最终是训练数据的产物，删除其中一个训练实例可能会影响生成的模型。当从训练数据中删除训练实例会显著改变模型的参数或预测时，我们称之为“有影响的”。通过识别有影响的培训实例，我们可以“调试”机器学习模型，更好地解释它们的行为和预测。

本章向您展示两种识别有影响的实例的方法，即删除诊断和影响函数。这两种方法都基于稳健的统计数据，该统计数据提供的统计方法受异常值或违反模型假设的影响较小。稳健统计还提供了测量数据的稳健估计（如平均估计或预测模型的权重）的方法。

想象一下，你想估计一下你所在城市的人们的平均收入，然后随机问十个街上的人他们挣多少钱。除了你的样本可能真的很糟糕之外，你的平均收入估计有多少会受到一个人的影响？为了回答这个问题，您可以通过省略单个答案来重新计算平均值，或者通过“影响函数”来数学推导平均值的影响方式。采用删除法，我们将平均值重新计算十次，每次忽略一个损益表，并测量平均估计值的变化量。一个巨大的变化意味着一个例子是非常有影响力的。第二种方法是用一个无穷小的权重来增加一个人的权重，这相当于统计或模型的一阶导数的计算。这种方法也称为“无穷小方法”或“影响函数”。答案是，顺便说一句，你的平均估计可能会受到一个答案的强烈影响，因为平均值与单个值成线性比例。一个更有力的选择是中位数（一半人赚得更多，另一半人赚得更少），因为即使你的样本中收入最高的人赚得更多十倍，结果的中位数也不会改变。

删除诊断和影响功能也可应用于机器学习模型的参数或预测，以更好地了解其行为或解释个别预测。在我们研究这两种寻找有影响的实例的方法之前，我们将研究离群值实例和有影响的实例之间的区别。

离群值

离群值是一个远离数据集中其他实例的实例。“远”是指距离，例如欧几里得距离，到所有其他实例的距离非常大。在一组新生儿中，体重6公斤的新生儿被认为是异常值。在一个以支票账户为主的银行账户数据集中，一个专门的贷款账户（大额负余额，很少有交易）将被视为异常值。下图显示了1维度分布的异常值。

图6.14：特征X遵循高斯分布，X=8处有一个异常值。

离群值可以是有趣的数据点（例如当异常值影响模型时，它也是一个有影响的实例。有影响的实例

一个有影响的实例是一个数据实例，它的删除对训练模型有很大的影响。

当模型被从训练数据中删除的特定实例重新训练时，模型参数或预测的更改越多，该实例的影响就越大。一个实例对训练模型是否有影响也取决于它对目标Y的值。下图显示了一个对线性回归模型有影响的实例。

图6.15：具有一个特征的线性模型。培训一次完整的数据，一次没有影响的实例。删除影响的实例会显著改变拟合的坡度（重量/系数）。

为什么有影响的实例有助于理解模型？

对解释性有影响的实例背后的关键思想是跟踪模型参数和预测，使其回到最初的位置：培训数据。学习者，即生成机器学习模型的算法，是一个函数，它获取由特征X和目标Y组成的训练数据，并生成机器学习模型。例如，决策树的学习者是一种选择分割特征和分割值的算法。神经网络的学习者使用反向传播来找到最佳权重。

图6.16：学习者从培训数据（功能和目标）中学习模型。该模型对新数据进行预测。

我们会询问如果在培训过程中从培训数据中删除实例，模型参数或预测将如何更改。这与其他可解释性方法形成了对比，这些方法分析了当我们操纵要预测的实例（如或）的特征时，预测是如何变化的。对于有影响的实例，我们不将模型视为固定的，而是将其视为训练数据的函数。有影响的实例帮助我们回答有关全球模型行为和个人预测的问题。模型参数或预测的最有影响的实例是什么？对于一个特定的预测，哪些是最有影响的实例？有影响的实例告诉我们模型可能会出现问题的实例，应该检查哪些培训实例是否存在错误，并给出模型的健壮性印象。如果单个实例对模型预测和参数有很大的影响，我们可能不信任模型。至少这会使我们进一步调查。

我们如何才能找到有影响力的实例？我们有两种测量影响的方法：第一种方法是从训练数据中删除实例，在减少的训练数据集上重新训练模型，并观察模型参数或预测的差异（单个或整个数据集）。第二个选项是通过根据模型参数的梯度近似参数更改来增加数据实例的权重。删除方法更容易理解和激发上权方法，因此我们从前者开始。

6.4.1删除诊断

统计学家已经在有影响的实例领域做了大量的研究，特别是对于（广义）线性回归模型。当你搜索“有影响的观察结果”时，第一个搜索结果是关于dfbeta和cook距离等度量的。dfbeta测量删除实例对模型参数的影响。Cook的距离（Cook，197）测量删除实例对模型预测的影响。对于这两个度量，我们必须重复地重新培训模型，每次都忽略单个实例。将模型的所有实例的参数或预测与模型的参数或预测进行比较，其中一个实例已从训练数据中删除。

DFBETA定义为：

\[dfbeta i=\beta-\beta ^（-i）\]

其中\（\beta\）是模型在所有数据实例上进行训练时的权重向量，而\（\beta^（-i）\）是模型在没有实例i的情况下进行训练时的权重向量。我会说非常直观。DFBETA仅适用于具有权重参数的模型，如逻辑回归或神经网络，但不适用于决策树、树集合、某些支持向量机等模型。

Cook距离是为线性回归模型而发明的，并且存在广义线性回归模型的近似值。Cook对训练实例的距离定义为当第i个实例从模型训练中移除时，预测结果的平方差之和。

\[d i=\frac \ sum j=1 ^n（\hat y（-i））^2 p\cdot mse]

其中，分子是有第i个实例和没有第i个实例的模型预测之间的平方差，在数据集上求和。分母是特征数p乘以均方误差。无论删除哪个实例，所有实例的分母都相同。库克的距离告诉我们，当我们从训练中移除第i个实例时，线性模型的预测输出会发生多大的变化。

我们能把库克的距离和dfbeta用于任何机器学习模型吗？dfbeta需要模型参数，因此此度量仅适用于参数化模型。库克的距离不需要任何模型参数。有趣的是，Cook的距离通常不在线性模型和广义线性模型的范围之外，但考虑到特定实例删除前后模型预测之间的差异的想法是非常普遍的。库克距离定义的一个问题是MSE，这对所有类型的预测模型（如分类）都没有意义。

对模型预测影响的最简单影响测量可写如下：

\[\text影响^（-i）=\frac

这个表达式基本上是库克距离的分子，用绝对差代替平方差相加。这是我做的一个选择，因为它对后面的例子有意义。删除诊断度量的一般形式包括选择一个度量（如预测结果），并计算在所有实例上训练的模型和删除实例时度量的差异。

我们可以很容易地将影响分解，为实例j的预测解释第i次培训实例的影响是什么：

\[\文本影响（-i）=\左\右（-i）\右]

这也适用于模型参数的差异或损失的差异。在下面的示例中，我们将使用这些简单的影响度量。

## 删除诊断示例

在下面的例子中，我们训练一个支持向量机来预测给定的风险因素，并测量哪些训练实例对整体和特定的预测最有影响。由于癌症预测是一个分类问题，因此我们将影响作为癌症预测概率的差异来衡量。当实例从模型训练中移除时，如果预测的概率在数据集中平均显著增加或减少，则实例具有影响。对所有858个培训实例的影响测量需要对所有数据进行一次模型培训，并在每次删除其中一个实例的情况下对模型进行858次重新培训（=培训数据的大小）。

最有影响的实例的影响度量约为0.01。0.01的影响意味着如果我们去掉第540个实例，预测的概率平均变化1个百分点。考虑到癌症的平均预测概率为6.4%，这是相当可观的。所有可能删除的影响度量的平均值为0.2个百分点。现在我们知道哪些数据实例对模型最有影响。这对于调试数据已经很有用了。是否存在问题实例？是否存在测量误差？有影响的实例是第一个应该检查错误的实例，因为其中的每个错误都强烈影响模型预测。

除了模型调试之外，我们可以学习一些东西来更好地理解模型吗？仅仅打印出前10个最有影响力的实例并不是很有用，因为它只是一个具有许多特性的实例表。所有将实例作为输出返回的方法只有在我们有好的方法来表示它们的情况下才有意义。但是，当我们问：有影响力的实例和没有影响力的实例有什么区别时，我们可以更好地理解哪些实例是有影响力的？我们可以将这个问题转化为回归问题，并将实例的影响建模为其特征值的函数。我们可以从本章中自由选择任何型号。对于这个例子，我选择了一个决策树（下图），它显示来自35岁及以上妇女的数据对支持向量机最有影响。在数据集中的所有女性中，858人中有153人年龄大于35岁。在这一章中，我们已经看到，40岁以后，癌症的预测概率急剧增加，而且年龄也是最重要的特征之一。影响分析告诉我们，当预测癌症的年龄更高时，模型变得越来越不稳定。这本身就是有价值的信息。这意味着这些实例中的错误会对模型产生很大的影响。

图6.17：模拟实例影响与其特征之间关系的决策树。树的最大深度设置为2。

这第一个影响分析揭示了最具影响力的总体情况。现在我们选择其中一个实例，即第7个实例，我们想通过找到最有影响的训练数据实例来解释预测。这就像一个反事实的问题：如果我们从培训过程中忽略了实例I，那么实例7的预测会发生什么变化？我们对所有实例重复此删除。然后，我们选择在训练中忽略实例7时，导致预测最大变化的训练实例，并用它们来解释该实例的模型预测。我选择解释这个预测，例如7，因为它是癌症预测概率最高的实例（7.35%），我认为这是一个有趣的案例，需要更深入地分析。我们可以返回前10个最有影响力的实例来预测作为表格打印的第7个实例。不是很有用，因为我们看不到太多。再次，通过分析影响实例和非影响实例的特点，找出影响实例和非影响实例之间的区别是更有意义的。我们使用一个经过训练的决策树来预测给定特征的影响，但实际上我们滥用它只是为了找到一个结构，而不是实际预测某些东西。下面的决策树显示了哪种训练实例对预测第7个实例最有影响。

图6.18：解释哪些实例对预测第7个实例最有影响的决策树。吸烟18.5年或更长时间的妇女的数据对第7例的预测有很大影响，绝对预测的平均变化为癌症概率的11.7个百分点。

吸烟或已吸烟18.5年或更长时间的妇女的数据实例对第7例的预测有很大影响。第7例背后的女人吸烟了34年。数据显示，12名女性（1.40%）吸烟18.5年或更长时间。在收集其中一名妇女的吸烟年限时所犯的任何错误都将对第7例的预测结果产生巨大影响。

当我们删除实例663时，预测会发生最极端的变化。该患者据称吸烟22年，与决策树的结果一致。如果删除实例663，第7个实例的预测概率从7.35%变为66.60%。

如果我们仔细看看最有影响的实例的特性，我们可以看到另一个可能的问题。数据显示，这名妇女28岁，已经吸烟22年了。要么是非常极端的情况，她6岁就开始抽烟，要么就是数据错误。我倾向于相信后者。在这种情况下，我们必须质疑数据的准确性。

这些示例显示了识别对调试模型有影响的实例是多么有用。该方法的一个问题是，模型需要针对每个训练实例进行重新训练。整个再培训过程可能非常缓慢，因为如果您有数千个培训实例，则必须对模型进行数千次再培训。假设模型需要一天的训练时间，并且您有1000个训练实例，那么计算有影响的实例（没有并行化）将需要近3年的时间。没人有时间做这个。在本章的其余部分中，我将向您展示一种不需要重新培训模型的方法。

6.4.2影响功能

你：我想知道一个训练实例对特定预测的影响。

研究：您可以删除培训实例，重新培训模型，并测量预测的差异。

你：太好了！但是你有没有一个方法可以让我不用再培训？这需要很多时间。研究：你有一个损失函数的模型，它的参数是两倍可微的吗？

你：我训练了一个神经网络来处理物流损失。所以是的。

研究：然后用影响函数来近似实例对模型参数和预测的影响。影响函数是衡量模型参数或预测对训练实例的依赖程度的指标。方法不是删除实例，而是通过一个非常小的步骤对丢失的实例进行加权。该方法包括利用梯度和黑森矩阵近似当前模型参数的损失。损失增加权重类似于删除实例。你：太好了，这就是我要找的！

Koh和Liang（2017年建议使用影响函数（一种稳健统计方法）来测量实例如何影响模型参数或预测。与删除诊断一样，影响函数跟踪模型参数和预测返回到负责的培训实例。然而，该方法并没有删除训练实例，而是近似于当实例在经验风险（训练数据损失之和）中被加权时，模型的变化程度。

影响函数的方法要求访问与模型参数相关的损失梯度，这只适用于机器学习模型的一个子集。逻辑回归、神经网络和支持向量机是合格的，像随机森林这样的基于树的方法是不合格的。影响函数有助于理解模型行为、调试模型和检测数据集中的错误。

以下部分解释了影响函数背后的直觉和数学。

## 影响函数背后的数学

影响函数背后的关键思想是通过无限小的步骤（epsilon）增加训练实例的损失，从而产生新的模型参数：

\[\hat \theta \epsilon，z=\arg \min \theta \i n \theta（1-\epsilon \frac n \sum i=1 l（z i，theta）

+\ epsilon l（z，theta）\]

其中\（\theta\）是模型参数向量，而\（\hat \ theta \ epsilon，z \）是用非常小的数字\（\epsilon\）对z进行加权后的参数向量。l是模型训练的损失函数，z是训练数据，z是我们想要增加权重来模拟其移除的训练实例。这个公式背后的直觉是：如果我们将训练数据中的某个特定实例（z_i）稍微增加一点（\（\ epsilon\）并相应地降低其他数据实例的权重，损失会有多大变化？参数向量是什么样子来优化这个新的组合损失？参数的影响函数，即上权训练实例Z对参数的影响，可计算如下。

\[I   \ Theta \\ \ Epsilon，Z \123\ \ \\123;  123; \θl（z，that \θ）\]

最后一个表达式\（\nabla \theta l（z，that \theta）是相对于上权训练实例参数的损失梯度。梯度是训练实例丢失的变化率。它告诉我们，当我们稍微改变模型参数时，损失会有多大的变化。梯度向量中的正项意味着相应模型参数的微小增加会增加损失，负项意味着参数的增加会减少损失。第一部分\（h^-1 \ that \ theta）是逆Hessian矩阵（损失相对于模型参数的二阶导数）。黑森矩阵是梯度变化率，或表示为损耗，它是损耗变化率的变化率。可以使用下列公式进行估算：

更非正式的是：黑森矩阵记录了损失在某一点上的弯曲程度。Hessian是一个矩阵，而不仅仅是一个向量，因为它描述了损失的曲率，曲率取决于我们观察的方向。如果你有许多参数，那么海森矩阵的实际计算是耗时的。Koh和Liang提出了一些有效计算的技巧，这超出了本章的范围。如上述公式所述，更新模型参数，相当于在估计模型参数周围形成二次展开后采取一个牛顿步骤。

这个影响函数公式背后的直觉是什么？公式来自于围绕参数形成二次展开式。这意味着我们实际上不知道，或者它太复杂了，无法计算实例z的损失在被删除/加权时究竟会有多大的变化。我们利用当前模型参数设置下的陡度（=梯度）和曲率（=黑森矩阵）的信息对函数进行局部近似。通过这种损失近似，我们可以计算出如果我们对实例z进行加权，新参数将大致是什么样子：

\[\Hat \Theta-Z \约\Hat \Theta-\Frac 1 n i \ Text up，参数（Z）]

近似参数向量基本上是原始参数减去z的损失梯度（因为我们想减少损失），用曲率（=乘以逆Hessian矩阵）缩放，用1除以n缩放，因为这是单个训练实例的权重。

下图显示了向上加权的工作原理。x轴显示了\（\theta\）参数的值，y轴显示了加权实例z对应的损失值。这里的模型参数是一维的，用于演示，但实际上通常是高维的。我们只移动1到n的方向上，以改善损失，例如z。我们不知道当我们删除z时损失会如何变化，但是使用损失的一阶和二阶导数，我们围绕当前模型参数创建这个二次近似，并假设真正的损失是如何表现的。

图6.19：通过在当前模型参数周围形成损耗的二次扩展，并将1/N移动到具有上加权实例z（y轴）的损耗改善最多的方向，更新模型参数（x轴）。如果我们删除Z并在减少的数据上对模型进行训练，则损失中实例Z的上浮近似于参数变化。

我们实际上不需要计算新的参数，但是我们可以使用影响函数来测量z对参数的影响。

当我们增加训练实例z的权重时，预测是如何变化的？我们既可以计算新参数，然后使用新参数化模型进行预测，也可以直接计算实例z对预测的影响，因为我们可以使用链规则计算影响：

\[\begin align*i up，loss（z，z test）&=\ left.\frac d l（z test，\hat \theta \epsilon，z）d \epsilon

\右\ epsilon=0 \ \左.\nabla \ theta l（z测试，\t hat \ theta）^t\frac d\that \ theta \ epsilon，z

；）\结束对齐*\]

这个方程的第一行意味着我们测量训练实例对某个预测（z_test \）的影响，当我们增加实例z的权重并获得新参数时，作为测试实例损失的变化。对于方程的第二行，我们应用了导数链规则，得到了测试实例损失的导数，它与参数乘以z对参数的影响。在第三行，我们将表达式替换为参数的影响函数。第三行中的第一个术语是测试实例相对于模型参数的梯度。

有一个公式是伟大的，科学和准确的方式显示事物。但我认为对公式的含义有一些直觉是非常重要的。（i_ text up，loss）的公式指出，训练实例z对实例预测的影响函数为“当我们对inst进行权重计算时，实例对模型参数变化的反应程度”乘以“参数变化的程度”。安斯Z“。阅读公式的另一种方法是：影响与训练和测试损失的梯度大小成正比。训练损失梯度越大，对参数的影响越大，对测试预测的影响越大。测试预测的梯度越高，对测试实例的影响就越大。整个结构也可以看作是训练和测试实例之间相似性的度量（由模型学习）。

这就是理论和直觉。下一节将解释如何应用影响函数。

## 影响函数的应用

影响函数有许多应用，其中一些已经在本章中介绍。

## 了解模型行为

不同的机器学习模型有不同的预测方法。即使两个模型具有相同的性能，它们根据特性进行预测的方式也可能非常不同，因此在不同的场景中失败。通过识别有影响的实例来了解模型的特定弱点，有助于在您的头脑中形成机器学习模型行为的“心理模型”。下图显示了一个例子，其中训练了支持向量机（SVM）和神经网络来区分狗和鱼的图像。对于这两个模型来说，最具影响力的鱼类典型形象的例子是非常不同的。对于SVM来说，如果实例在颜色上相似，那么它们是有影响的。对于神经网络来说，如果实例在概念上相似，那么它们是有影响的。对于神经网络来说，甚至一张狗的图像也是最有影响的图像之一，这表明它学习了概念，而不是颜色空间中的欧几里得距离。

图6.20：狗还是鱼？对于与测试图像颜色相似的支持向量机预测（中行）图像，影响最大。对于神经网络预测（底排），不同环境下的鱼类影响最大，但也有狗的图像（右上角）。Koh和Liang的作品（2017年）。

## 处理域不匹配/调试模型错误

处理域不匹配与更好地理解模型行为密切相关。域不匹配意味着训练数据和测试数据的分布是不同的，这可能导致模型在测试数据上的性能不佳。影响函数可以识别导致错误的训练实例。假设您为接受过手术的患者培训了一个预测模型。所有这些病人都来自同一家医院。现在，您在另一家医院使用该模型，发现它对许多患者都不起作用。当然，你假设两个医院的病人不同，如果你看他们的数据，你会发现他们在很多方面都不同。但是什么是“破坏”模型的特性或实例呢？在这里，有影响力的例子也是回答这个问题的好方法。你可以选取一个新的病人，模型对他们做出了错误的预测，找到并分析最有影响的例子。例如，这可能表明第二家医院平均有老年患者，从培训数据来看，最有影响的例子是第一家医院的少数老年患者，而该模型仅仅缺少学习预测该亚组的数据。结论是，为了在第二家医院更好地工作，该模型需要对更多年龄较大的患者进行培训。

## 修复培训数据

如果您对可以检查正确性的培训实例数量有限制，那么如何进行有效的选择呢？最好的方法是选择最有影响力的实例，因为根据定义，它们对模型的影响最大。即使您有一个具有明显不正确值的实例，如果该实例不具有影响，并且您只需要预测模型的数据，那么最好检查具有影响的实例。例如，您培训一个模型来预测患者是应该住院还是应该提前出院。您真的想确保模型是健壮的，并做出正确的预测，因为错误地释放患者可能会产生不良后果。患者记录可能非常混乱，因此您对数据的质量没有完全的信心。但是检查病人信息和纠正它是非常耗时的，因为一旦你报告了需要检查的病人，医院实际上需要派人更仔细地查看所选病人的记录，这些记录可能是手写的，并放在一些拱门中。伊夫。检查患者的数据可能需要一个小时或更长时间。考虑到这些成本，只检查一些重要的数据实例是有意义的。最好的方法是选择对预测模型影响较大的患者。Koh和Liang（2017）表明，这种类型的选择比随机选择或损失最大或分类错误的选择效果要好得多。

6.4.3识别有影响的实例的优势

删除诊断和影响函数的方法与A look at important instances中提出的基于特征扰动的方法有很大的不同，它强调了训练数据在学习过程中的作用。这使得影响函数和删除诊断成为机器学习模型的最佳调试工具之一。在本书中介绍的技术中，它们是唯一直接帮助识别应该检查错误的实例的技术。

删除诊断是模型不可知论，这意味着该方法可以应用于任何模型。基于导数的影响函数也可以应用于广泛的模型。

我们可以使用这些方法来比较不同的机器学习模型，更好地了解它们的不同行为，而不仅仅是比较预测性能。

在本章中我们没有讨论过这个主题，但是通过衍生工具的影响函数也可以用来创建对抗训练数据。这些实例的操作方式使得当模型在这些被操作的实例上进行培训时，模型无法正确地预测某些测试实例。与中的方法不同的是，攻击发生在训练期间，也称为中毒攻击。如果您感兴趣，请阅读Koh和Liang（2017）的论文。

对于缺失诊断和影响函数，我们考虑了预测中的差异，对于影响函数，考虑了损失的增加。但是，实际上，这种方法可以推广到任何形式的问题：“当我们删除或增加实例z的权重时，会发生什么？“，在这里你可以用你的欲望模型的任何功能来填充”…“。您可以分析培训实例对模型的整体损失有多大的影响。您可以分析培训实例对功能重要性的影响程度。可以分析培训实例对培训A时为第一个拆分选择的功能有多大影响。

6.4.4识别有影响的实例的缺点

删除诊断计算起来非常昂贵，因为它们需要重新培训。但历史表明计算机资源在不断增加。20年前的一个计算在资源方面是不可想象的，可以很容易地用你的智能手机进行。您可以在笔记本电脑上以秒/分钟的速度训练具有数千个训练实例和数百个参数的模型。因此，假设删除诊断在10年内即使对大型神经网络也能毫无问题地工作，这并不是一个大的飞跃。

影响函数是删除诊断的一个很好的替代方法，但只适用于参数可微的模型，如神经网络。它们不适用于基于树的方法，如随机森林、增强树或决策树。即使你有带参数和损失函数的模型，损失也可能是不可微的。但对于最后一个问题，有一个诀窍：当基础模型使用铰链损失而不是某些可微分损失时，使用可微分损失代替计算影响。对于影响函数，损失被问题损失的平滑版本所取代，但是模型仍然可以用非光滑损失进行训练。

影响函数只是近似的，因为该方法围绕参数形成二次展开。近似值可能是错误的，当移除实例时，实例的影响实际上更高或更低。Koh和Liang（2017）举例说明，通过影响函数计算的影响接近于删除实例后实际重新约束模型时获得的影响度量。但不能保证近似值总是如此接近。

我们称之为有影响力或无影响力的实例的影响度量没有明确的界限。根据影响对实例进行分类是很有用的，但最好的方法不仅是对实例进行分类，而且实际上是区分有影响的实例和无影响的实例。例如，如果您为一个测试实例确定10个最有影响力的培训实例，其中一些可能不具有影响力，因为例如，只有前3个才真正具有影响力。

影响措施只考虑单个实例的删除，不考虑同时删除多个实例。较大的数据实例组可能具有一些交互作用，这些交互作用对模型训练和预测有很大影响。但问题在于组合数学：从数据中删除单个实例有n种可能性。有n次（n-1）可能从训练数据中删除两个实例。有n次（n-1）次（n-2）的可能性删除三个…我想你可以看到这是去哪里，有太多的组合。

6.4.5软件和备选方案

删除诊断很容易实现。看看我在本章中写的例子。

对于线性模型和广义线性模型，许多影响度量如Cook距离在统计包的R中实现。

Koh和Liang从他们的论文中发布了影响函数的python代码。

太好了！不幸的是，它是本文的“唯一”代码，而不是维护和文档化的Python模块。代码集中在TensorFlow库上，因此您不能使用它直接用于使用其他框架（如sci kit learn）的黑盒模型。

Keita Kurita写了一篇帮助我更好地理解Koh和Liang的论文。博客文章对黑匣子模型的影响函数背后的数学进行了更深入的探讨，并讨论了一些有效实现该方法的数学“技巧”。

![img](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image001.gif)

\1.    库克，R.丹尼斯。“线性回归中影响观察的检测”，《技术计量学》19.1（1977）：15-18。

\2.    Koh、庞伟和Percy Liang。“通过影响函数了解黑盒预测”，arxiv预印arxiv:1703.04730（2017）。

